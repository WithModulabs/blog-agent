import os
import requests
import streamlit as st
from bs4 import BeautifulSoup
from requests.exceptions import SSLError, RequestException

from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
import trafilatura
from urllib.parse import urlparse, urljoin, parse_qs


def get_llm(temperature: float = 0.7):
    model_provider = st.session_state.get("model_provider", "OpenAI")
    
    if model_provider == "OpenAI":
        api_key = st.session_state.get("openai_api_key")
        if not api_key:
            st.error("OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        os.environ["OPENAI_API_KEY"] = api_key
        try:
            return ChatOpenAI(model="gpt-4o", temperature=temperature)
        except Exception as e:
            st.error(f"OpenAI LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
            
    elif model_provider == "Gemini":
        api_key = st.session_state.get("gemini_api_key")
        if not api_key:
            st.error("Google API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        try:
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-flash", 
                google_api_key=api_key,
                temperature=temperature,
                convert_system_message_to_human=True
            )
        except Exception as e:
            st.error(f"Gemini LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None

    elif model_provider == "Claude":
        api_key = st.session_state.get("anthropic_api_key")
        if not api_key:
            st.error("Anthropic API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        try:
            return ChatAnthropic(
                model="claude-4-sonnet",
                api_key=api_key,
                temperature=temperature,
            )
        except Exception as e:
            st.error(f"Claude LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    return None


def generate_image_with_gemini(prompt: str, api_key: str):
    """Pollinations.aië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±

    ë¬´ë£Œ AI ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤ì¸ Pollinations.aië¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    API í‚¤ê°€ í•„ìš” ì—†ìœ¼ë©°, URLì„ í†µí•´ ì§ì ‘ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        import urllib.parse
        import time

        # Pollinations.ai API ì‚¬ìš© (ë¬´ë£Œ, API í‚¤ ë¶ˆí•„ìš”)
        # í”„ë¡¬í”„íŠ¸ë¥¼ URL ì¸ì½”ë”©
        encoded_prompt = urllib.parse.quote(prompt)

        # Pollinations.ai ì´ë¯¸ì§€ ìƒì„± URL
        # seedë¥¼ ì¶”ê°€í•˜ì—¬ ë§¤ë²ˆ ë‹¤ë¥¸ ì´ë¯¸ì§€ ìƒì„±
        seed = int(time.time())
        image_url = f"https://image.pollinations.ai/prompt/{encoded_prompt}?seed={seed}&width=1024&height=1024&nologo=true"

        # Pollinations.aiëŠ” ì²« ìš”ì²­ ì‹œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦¼
        # HEAD ìš”ì²­ ëŒ€ì‹  ì§ì ‘ URLì„ ë°˜í™˜ (ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì˜¬ ë•Œ ìƒì„±ë¨)
        st.info("ğŸ¨ Pollinations.aië¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ìƒì„± ì¤‘... (ì²« ë¡œë”© ì‹œ 10-20ì´ˆ ì†Œìš”)")

        # ì´ë¯¸ì§€ ìƒì„±ì„ íŠ¸ë¦¬ê±°í•˜ê¸° ìœ„í•´ GET ìš”ì²­ì„ ë³´ë‚´ë˜,
        # íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•´ë„ URLì€ ìœ íš¨í•˜ë¯€ë¡œ ë°˜í™˜
        try:
            # ì´ë¯¸ì§€ ìƒì„± íŠ¸ë¦¬ê±° (ìµœëŒ€ 40ì´ˆ ëŒ€ê¸°)
            response = requests.get(image_url, timeout=40, stream=True)
            if response.status_code == 200:
                st.success("âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
                return image_url
        except requests.Timeout:
            # íƒ€ì„ì•„ì›ƒì´ ë°œìƒí•´ë„ URLì€ ìœ íš¨í•¨
            st.warning("â³ ì´ë¯¸ì§€ ìƒì„± ì¤‘... URLì€ ìœ íš¨í•˜ë©° ì ì‹œ í›„ í‘œì‹œë©ë‹ˆë‹¤.")
            return image_url
        except Exception:
            # ë‹¤ë¥¸ ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ URL ìì²´ëŠ” ìœ íš¨í•  ìˆ˜ ìˆìŒ
            return image_url

        return image_url

    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ URL ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def _session():
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    })
    return s

def _scrape_naver_blog(url: str):
    s = _session()
    try:
        r = s.get(url, timeout=20)
        r.raise_for_status()
    except RequestException as e:
        return "", f"URL ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    soup = BeautifulSoup(r.content, "html.parser")

    frame = soup.find("iframe", {"id": "mainFrame"}) or soup.find("frame", {"id": "mainFrame"})
    if not frame or not frame.get("src"):
        return "", "ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    inner_url = urljoin("https://blog.naver.com", frame.get("src"))
    try:
        r2 = s.get(inner_url, timeout=20)
        r2.raise_for_status()
    except RequestException as e:
        return "", f"URL ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    inner = BeautifulSoup(r2.content, "html.parser")

    title_candidates = [
        ".se-title-text", ".se_title_text", "h3.se_textarea", "#title_1", "h3#postTitleText"
    ]
    title = ""
    for sel in title_candidates:
        el = inner.select_one(sel)
        if el and el.get_text(strip=True):
            title = el.get_text(strip=True)
            break
    if not title:
        title = inner.title.get_text(strip=True) if inner.title else ""

    container = inner.select_one(".se-main-container") or inner.select_one("#postViewArea")
    if container:
        for tag in container(["script","style","nav","footer","aside","form"]):
            tag.decompose()
        text = container.get_text(separator="\n", strip=True)
        return title, text if text else "ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    extracted = trafilatura.extract(r2.text)
    if extracted:
        return title, extracted

    return title, "ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def scrape_web_content(url: str):
    parsed = urlparse(url)
    host = parsed.netloc.lower()
    if "blog.naver.com" in host or "m.blog.naver.com" in host:
        return _scrape_naver_blog(url)

    s = _session()
    try:
        r = s.get(url, timeout=20)
        r.raise_for_status()
    except SSLError:
        r = s.get(url, timeout=20, verify=False)
        r.raise_for_status()
    except RequestException as e:
        return "", f"URL ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    title = ""
    try:
        soup = BeautifulSoup(r.content, "html.parser")
        title = soup.title.get_text(strip=True) if soup.title else ""
    except Exception:
        pass

    extracted = trafilatura.extract(r.text)
    if extracted:
        return title, extracted
    return title, "ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
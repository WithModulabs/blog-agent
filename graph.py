import json

import streamlit as st
from typing import List, TypedDict

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_tavily import TavilySearch
from langgraph.graph import StateGraph, END
from openai import OpenAI

# tools.pyì—ì„œ í•¨ìˆ˜ ì„í¬íŠ¸
from tools import get_llm, scrape_web_content

# --- ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜ ---
class AgentState(TypedDict):
    url: str
    scraped_content: str
    seo_analysis: str
    seo_tags: List[str]
    draft_post: str
    final_title: str
    final_subheadings: List[str]
    naver_seo_subtitles: List[str]
    image_prompt: str
    image_url: str
    blog_index: int
    subtitle_image_prompts: List[str]
    subtitle_image_urls: List[str]
    image_keywords: List[str]
    messages: List[BaseMessage]

# --- ì—ì´ì „íŠ¸ ë° ë…¸ë“œ ì •ì˜ ---
def researcher_node(state: AgentState):
    st.write("â–¶ï¸ ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: URL ì½˜í…ì¸  ë¶„ì„ ì‹œì‘...")
    url = state['url']
    scraped_content = scrape_web_content(url)
    
    failure_keywords = ["ì˜¤ë¥˜ ë°œìƒ", "ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ìŠ¤í¬ë©ì´ ê¸ˆì§€ëœ ê¸€"] 
    
    if any(keyword in scraped_content for keyword in failure_keywords):
        st.error(f"âš ï¸ {scraped_content}")
        return {
            "scraped_content": f"ë¶„ì„ ì‹¤íŒ¨: {scraped_content}",
        }
        
    st.success("âœ… ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ!")
    return {
        "scraped_content": scraped_content,
        "messages": [HumanMessage(content=f"URL '{url}'ì˜ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ.")]
    }

def seo_specialist_node(state: AgentState):
    st.write("â–¶ï¸ SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ë„¤ì´ë²„ SEO ì „ëµ ë¶„ì„ ì¤‘...")
    scraped_content = state['scraped_content']
    search_query = "2025ë…„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ìµœì í™” ì „ëµ"
    tavily_api_key = st.session_state.get("tavily_api_key")

    if not tavily_api_key:
        st.error("âŒ Tavily API Keyê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {"scraping_status": "Failure", "seo_analysis": "Tavily API Key ì—†ìŒ", "seo_tags": []}

    try:
        tavily_tool = TavilySearch(max_results=3, tavily_api_key=tavily_api_key)
        search_results = tavily_tool.invoke({"query": search_query})
        # Extract content from search results
        seo_trends = ""
        if search_results and "results" in search_results:
            for result in search_results["results"]:
                seo_trends += f"ì œëª©: {result.get('title', '')}\në‚´ìš©: {result.get('content', '')}\n\n"
        else:
            seo_trends = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        st.error(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        seo_trends = ""

    prompt = ChatPromptTemplate.from_messages([
         ("system", 
         """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
         ì£¼ì–´ì§„ ì›ë³¸ ì½˜í…ì¸ ì™€ ìµœì‹  SEO íŠ¸ë Œë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
         ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
         [ë¶„ì„ ë° ì „ëµ]
         - (ì—¬ê¸°ì— ì½˜í…ì¸  ê¸°ë°˜ SEO ì „ëµê³¼ í‚¤ì›Œë“œ ë¶„ì„ ë‚´ìš©ì„ ì„œìˆ )
         
         [ì¶”ì²œ íƒœê·¸]
         íƒœê·¸1, íƒœê·¸2, íƒœê·¸3, ... (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ 30ê°œì˜ íƒœê·¸)"""),
        ("human", 
         "**ìµœì‹  ë„¤ì´ë²„ SEO íŠ¸ë Œë“œ:**\n{seo_trends}\n\n"
         "**ë¶„ì„í•  ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"),
    ])
    
    llm = get_llm()
    if llm is None:
        return {"scraping_status": "Failure", "seo_analysis": "LLM ì—†ìŒ", "seo_tags": []}

    chain = prompt | llm
    response = chain.invoke({
        "seo_trends": seo_trends,
        "scraped_content": scraped_content[:4000]
    })
    
    analysis_text = response.content
    try:
        tags_part = analysis_text.split("[ì¶”ì²œ íƒœê·¸]")[1].strip()
        tags = [tag.strip() for tag in tags_part.split(",") if tag.strip()]
    except IndexError:
        tags = []
    
    st.success("âœ… SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ì „ëµ ë¶„ì„ ë° íƒœê·¸ ìƒì„± ì™„ë£Œ!")
    return {"seo_analysis": analysis_text, "seo_tags": tags}

def writer_node(state: AgentState):
    st.write("â–¶ï¸ ì‘ì„±ê°€ ì—ì´ì „íŠ¸: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    scraped_content = state['scraped_content']
    seo_analysis = state['seo_analysis']
    
    title_prompt = ChatPromptTemplate.from_template(
        "ì£¼ì–´ì§„ SEO ë¶„ì„ê³¼ ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ì œëª© 1ê°œë§Œ ìƒì„±í•´ì£¼ì„¸ìš”. (ì¶”ê°€ ì„¤ëª… ì—†ì´ ì œëª©ë§Œ ì¶œë ¥)\n\n"
        "**SEO ë¶„ì„:**\n{seo_analysis}\n\n"
        "**ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"
    )
    
    llm = get_llm()
    if llm is None:
        return {"draft_post": "LLM ì—†ìŒ", "final_title": "", "final_subheadings": [], "naver_seo_subtitles": []}

    title_chain = title_prompt | llm
    main_title = title_chain.invoke({
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:2000]
    }).content.strip().replace('"', '')

    # ë„¤ì´ë²„ SEO ìµœì í™” ë¶€ì œëª© ìƒì„±
    subtitle_prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¸”ë¡œê·¸ ì œëª©ê³¼ SEO ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEOì— ìµœì í™”ëœ ë¶€ì œëª© 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        **ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë¶€ì œëª© ì‘ì„± ê°€ì´ë“œ:**
        - ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ë˜ë„ë¡ ì‘ì„±
        - í´ë¦­ì„ ìœ ë„í•˜ëŠ” ë§¤ë ¥ì ì¸ ë¬¸êµ¬ ì‚¬ìš© (ì˜ˆ: "ê¼­ ì•Œì•„ì•¼ í• ", "ì™„ë²½ ê°€ì´ë“œ", "ì‹¤ì œ í›„ê¸°")
        - êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ì‹œê°„ í‘œí˜„ í¬í•¨ (ì˜ˆ: "3ê°€ì§€ ë°©ë²•", "10ë¶„ ë§Œì—", "2025ë…„ ìµœì‹ ")
        - ê°ì •ì  ì–´í•„ì´ë‚˜ í˜¸ê¸°ì‹¬ ìœ ë°œ ìš”ì†Œ ì¶”ê°€
        - 20-30ì ë‚´ì™¸ë¡œ ì ì ˆí•œ ê¸¸ì´ ìœ ì§€
        - ê° ë¶€ì œëª©ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì¸¡ë©´ì„ ë‹¤ë£¨ë„ë¡ êµ¬ì„±
        
        ê° ë¶€ì œëª©ì€ í•œ ì¤„ì”© ë²ˆí˜¸ ì—†ì´ ì¶œë ¥í•˜ì„¸ìš”.
        
        **ë©”ì¸ ì œëª©:** {main_title}
        
        **SEO ë¶„ì„:**
        {seo_analysis}"""
    )
    
    subtitle_chain = subtitle_prompt | llm
    subtitle_response = subtitle_chain.invoke({
        "main_title": main_title,
        "seo_analysis": seo_analysis
    }).content
    
    naver_seo_subtitles = [line.strip() for line in subtitle_response.split('\n') if line.strip() and not line.strip().startswith('**')]

    draft_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì œëª©, SEO ë¶„ì„, ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ì¹œê·¼í•œ ì–´ì¡°ì˜ ë§¤ë ¥ì ì¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‚´ìš©ì€ ì„œë¡ , ë³¸ë¡ (ì†Œì œëª© ## ì‚¬ìš©), ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”."),
        ("human", "**ì œëª©:** {main_title}\n\n**SEO ë¶„ì„:**\n{seo_analysis}\n\n**ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"),
    ])
    
    draft_chain = draft_prompt | llm
    draft_post = draft_chain.invoke({
        "main_title": main_title,
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:4000]
    }).content
    
    subheadings = [line.replace('## ', '').strip() for line in draft_post.split('\n') if line.startswith('## ')]
            
    st.success("âœ… ì‘ì„±ê°€ ì—ì´ì „íŠ¸: í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ!")
    return {"draft_post": draft_post, "final_title": main_title, "final_subheadings": subheadings, "naver_seo_subtitles": naver_seo_subtitles}

def blog_indexer_node(state: AgentState):
    """ë¸”ë¡œê·¸ ì§€ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ì—ì´ì „íŠ¸"""
    st.write("â–¶ï¸ ë¸”ë¡œê·¸ ì§€ìˆ˜ ì—ì´ì „íŠ¸")
    st.success("âœ… ë¸”ë¡œê·¸ ì§€ìˆ˜ ê³„ì‚° ì¤‘...")

    draft_post = state["draft_post"]

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ë¶„ì„í•˜ì—¬ ë¸”ë¡œê·¸ ì§€ìˆ˜(Blog Index)ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ 10ê°œ í•­ëª©ì„ ê°ê° 0-10ì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ ì´ 100ì  ë§Œì ìœ¼ë¡œ ì±„ì í•˜ê³ , ê° í•­ëª©ë³„ í‰ê°€ ê·¼ê±°ì™€ ê°œì„ ì ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

## í‰ê°€ ê¸°ì¤€

### 1. ê²€ìƒ‰ ìµœì í™” ì œëª© ì‘ì„± 
- í•µì‹¬ í‚¤ì›Œë“œê°€ ì•ë¶€ë¶„ì— ìœ„ì¹˜í•˜ëŠ”ê°€? 
- ìˆ«ì, ì‹œê°„, ì§€ì—­ëª…ì„ í™œìš©í–ˆëŠ”ê°€? 
- í´ë¦­ì„ ìœ ë„í•˜ëŠ” ê°ì • ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? 

### 2. ì²« ë¬¸ë‹¨ì—ì„œ í•µì‹¬ ìš”ì•½ 
- 3ì¤„ ì´ë‚´ì— ê¸€ ì „ì²´ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì •ë¦¬ë˜ì—ˆëŠ”ê°€? 
- ì§ˆë¬¸í˜•ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ í˜¸ê¸°ì‹¬ì„ ìê·¹í•˜ëŠ”ê°€? 

### 3. ë…ì ê³µê° í¬ì¸íŠ¸ í™•ë³´ 
- ì‹¤ì œ ì‚¬ë¡€, ê²½í—˜ë‹´, ì—í”¼ì†Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? 
- "ì €ë„ ì²˜ìŒì—” ëª°ëëŠ”ë°â€¦" ê°™ì€ í†¤ìœ¼ë¡œ ì‹ ë¢°ê°ì„ í˜•ì„±í•˜ëŠ”ê°€? 

### 4. ë³¸ë¬¸ êµ¬ì¡°í™” 
- ì†Œì œëª©ì— í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? 
- ëª©ë¡/ë²ˆí˜¸ë¥¼ í™œìš©í•˜ì—¬ ê°€ë…ì„±ì„ ê°•í™”í–ˆëŠ”ê°€? 
- ê¸´ ë¬¸ì¥ì„ 2~3ì¤„ë¡œ ëŠì–´ ì¼ëŠ”ê°€? 

### 5. ê¾¸ì¤€í•œ êµ¬ë…ì ìœ ì…ì„ ìœ„í•œ ì‹œë¦¬ì¦ˆí™” 
- ë‹¨ë°œì„±ì´ ì•„ë‹Œ ì—°ì¬ ì‹œë¦¬ì¦ˆë¡œ êµ¬ì„±ë˜ì—ˆëŠ”ê°€? 
- ì˜ˆ: "ì´ˆë³´ìë¥¼ ìœ„í•œ â—‹â—‹ 1í¸", "ì‹¤ì „ ì‘ìš© 2í¸"

### 6. ë‚´ë¶€ ë§í¬ & ì™¸ë¶€ ë§í¬ ì „ëµ 
- ë¸”ë¡œê·¸ ë‚´ ë‹¤ë¥¸ ê¸€ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ê°€? 
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ì¶œì²˜ 1~2ê°œ ì¸ìš©

### 7. ì´ë¯¸ì§€ í™œìš©ë²• 
- ê¸€ë‹¹ ìµœì†Œ 3ì¥ ì´ìƒì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í–ˆëŠ”ê°€? 
- í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê·¸ë¦¼ íŒŒì¼ëª…ì„ ì‘ì„±í–ˆëŠ”ê°€? 
- ALT í…ìŠ¤íŠ¸ì— ì„¤ëª…ì„ ì¶”ê°€í–ˆëŠ”ê°€? 

### 8. CTA(Call To Action) ì‚½ì… 
- ê³µê°/êµ¬ë…/ì´ì›ƒì¶”ê°€ë¥¼ ìœ ë„í•˜ëŠ” ë¬¸êµ¬ê°€ ìˆëŠ”ê°€? 
- ëŒ“ê¸€ì„ ìœ ë„í•˜ëŠ” ì§ˆë¬¸ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? 

### 9. ë©”íƒ€ë°ì´í„°ì™€ íƒœê·¸ ìµœì í™” 
- ê¸€ì˜ ì¹´í…Œê³ ë¦¬, í•´ì‹œíƒœê·¸ê°€ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ”ê°€? 
- ë¶ˆí•„ìš”í•œ íƒœê·¸ ë‚¨ë°œì€ í”¼í•˜ê³  í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œë§Œ ì§‘ì¤‘í•˜ì—¬ íƒœê·¸ë¥¼ ì„¤ì •í–ˆëŠ”ê°€?

### 10. ì½˜í…ì¸  ì°¨ë³„í™” ìš”ì†Œ ì¶”ê°€ 
- ì§ì ‘ ì´¬ì˜í•œ ì‚¬ì§„, ì¸í¬ê·¸ë˜í”½, í‘œ, ì°¨íŠ¸ë¥¼ í™œìš©í–ˆëŠ”ê°€? 
- ë‹¨ìˆœ ìš”ì•½í˜•ì´ ì•„ë‹Œ ê²½í—˜+ì¸ì‚¬ì´íŠ¸ë¥¼ ë‹´ì•„ ë…ì°½ì„±ì„ ê°•í™”í–ˆëŠ”ê°€? 

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

```json
{{"blog_index": {{"total_score": 0}}}}
```

ìœ„ì˜ í‰ê°€ ê¸°ì¤€ì— ë”°ë¼ ì£¼ì–´ì§„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì„ ë¶„ì„í•˜ê³ , ë°˜ë“œì‹œ ìœ„ì˜ JSON í˜•ì‹ìœ¼ë¡œ ë¸”ë¡œê·¸ ì§€ìˆ˜ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”."""),
        ("human", "ë‹¤ìŒ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì˜ ë¸”ë¡œê·¸ ì§€ìˆ˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{draft_post}")
    ])

    llm = get_llm()
    if llm is None:
        return {"blog_index": 0}

    try:
        chain = prompt | llm
        response = chain.invoke({"draft_post": draft_post})

        # JSON ì‘ë‹µ íŒŒì‹±
        content = response.content

        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        if "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            json_content = content[json_start:json_end].strip()
        elif "{" in content and "}" in content:
            json_start = content.find("{")
            json_end = content.rfind("}") + 1
            json_content = content[json_start:json_end]
        else:
            json_content = content

        blog_index_full = json.loads(json_content)
        blog_index = blog_index_full.get("blog_index", {}).get("total_score", 0)

        st.success(f"âœ… ë¸”ë¡œê·¸ ì§€ìˆ˜ ê³„ì‚° ì™„ë£Œ. {blog_index}ì ")
        return {"draft_post": f"{draft_post}\n\n**ë¸”ë¡œê·¸ ì§€ìˆ˜**:{blog_index}", "blog_index": blog_index}

    except Exception as e:
        st.error(f"âŒ ë¸”ë¡œê·¸ ì§€ìˆ˜ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return {"blog_index": 0}

def art_director_node(state: AgentState):
    st.write("â–¶ï¸ ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    title = state['final_title']
    subtitles = state.get('naver_seo_subtitles', [])
    
    # ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ì„œëŠ” OpenAI í‚¤ê°€ ë°˜ë“œì‹œ í•„ìš”
    if not st.session_state.get("openai_api_key"):
        st.warning("âš ï¸ ì´ë¯¸ì§€ ìƒì„±(DALL-E)ì„ ìœ„í•´ì„œëŠ” OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return {"image_prompt": "", "image_url": "", "subtitle_image_prompts": [], "subtitle_image_urls": [], "image_keywords": []}

    # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±ì€ ì‚¬ìš©ìê°€ ì„ íƒí•œ LLM ì‚¬ìš©
    prompt_generator_llm = get_llm()
    if prompt_generator_llm is None:
        return {"image_prompt": "", "image_url": "", "subtitle_image_prompts": [], "subtitle_image_urls": [], "image_keywords": []}

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keyword_template = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ë¸”ë¡œê·¸ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 2ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì–¸ë”ìŠ¤ì½”ì–´(_)ë¡œ ì—°ê²°í•´ì„œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        "ì˜ˆ: 'ë§›ì§‘_í›„ê¸°' ë˜ëŠ” 'ì—¬í–‰_íŒ' í˜•ì‹ìœ¼ë¡œ\n"
        "ì œëª©: {title}"
    )
    keyword_chain = keyword_template | prompt_generator_llm
    keywords_response = keyword_chain.invoke({"title": title}).content.strip()
    image_keywords = [kw.strip() for kw in keywords_response.split('_')[:2]]
    
    try:
        client = OpenAI(api_key=st.session_state.get("openai_api_key"))
        
        # 1. ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± (ì œëª© ê¸°ë°˜)
        st.write("  ğŸ“¸ ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        main_prompt_template = ChatPromptTemplate.from_template("ë¸”ë¡œê·¸ ì œëª© '{title}'ì— ì–´ìš¸ë¦¬ëŠ” DALL-E 3 ì´ë¯¸ì§€ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.")
        main_chain = main_prompt_template | prompt_generator_llm
        main_image_prompt = main_chain.invoke({"title": title}).content
        
        main_response = client.images.generate(
            model="dall-e-3", prompt=main_image_prompt, size="1024x1024", quality="standard", n=1
        )
        main_image_url = main_response.data[0].url
        
        # 2. ë¶€ì œëª© ê¸°ë°˜ ì´ë¯¸ì§€ 3ê°œ ìƒì„±
        st.write("  ğŸ¨ ë¶€ì œëª© ê¸°ë°˜ ì´ë¯¸ì§€ 3ê°œ ìƒì„± ì¤‘...")
        subtitle_prompts = []
        subtitle_urls = []
        
        for i, subtitle in enumerate(subtitles[:3], 1):
            st.write(f"    â€¢ ì´ë¯¸ì§€ {i+1} ìƒì„± ì¤‘...")
            subtitle_prompt_template = ChatPromptTemplate.from_template(
                "ë¸”ë¡œê·¸ ë¶€ì œëª© '{subtitle}'ì— ì–´ìš¸ë¦¬ëŠ” DALL-E 3 ì´ë¯¸ì§€ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜."
            )
            subtitle_chain = subtitle_prompt_template | prompt_generator_llm
            subtitle_image_prompt = subtitle_chain.invoke({"subtitle": subtitle}).content
            subtitle_prompts.append(subtitle_image_prompt)
            
            subtitle_response = client.images.generate(
                model="dall-e-3", prompt=subtitle_image_prompt, size="1024x1024", quality="standard", n=1
            )
            subtitle_urls.append(subtitle_response.data[0].url)
        
        st.success("âœ… ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ 4ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
        return {
            "image_prompt": main_image_prompt, 
            "image_url": main_image_url,
            "subtitle_image_prompts": subtitle_prompts,
            "subtitle_image_urls": subtitle_urls,
            "image_keywords": image_keywords
        }
        
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "image_prompt": main_image_prompt if 'main_image_prompt' in locals() else "", 
            "image_url": "",
            "subtitle_image_prompts": [],
            "subtitle_image_urls": [],
            "image_keywords": image_keywords
        }

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def should_continue(state: AgentState):
    if "ë¶„ì„ ì‹¤íŒ¨:" in state.get('scraped_content', ''):
        return "end_process"
    else:
        return "continue_to_seo"

def build_graph():
    workflow = StateGraph(AgentState)
    
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("seo_specialist", seo_specialist_node)
    workflow.add_node("writer", writer_node)
    workflow.add_node("art_director", art_director_node)
    workflow.add_node("blog_indexer", blog_indexer_node)

    workflow.set_entry_point("researcher")
    workflow.add_conditional_edges(
        "researcher",
        should_continue,
        {"continue_to_seo": "seo_specialist", "end_process": END}
    )
    workflow.add_edge("seo_specialist", "writer")
    workflow.add_edge("writer", "blog_indexer")
    workflow.add_edge("blog_indexer", "art_director")
    workflow.add_edge("art_director", END)
    
    return workflow.compile()
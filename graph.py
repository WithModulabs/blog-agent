import streamlit as st
from typing import List, TypedDict

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_tavily import TavilySearch
from langgraph.graph import StateGraph, END
from openai import OpenAI

from tools import get_llm, scrape_web_content


class AgentState(TypedDict):
    url: str
    scraped_content: str
    seo_analysis: str
    seo_tags: List[str]
    draft_post: str
    final_title: str
    final_subheadings: List[str]
    naver_seo_subtitles: List[str]
    image_prompt: str
    image_url: str
    subtitle_image_prompts: List[str]
    subtitle_image_urls: List[str]
    image_keywords: List[str]
    messages: List[BaseMessage]


def researcher_node(state: AgentState):
    st.write("â–¶ï¸ ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: URL ì½˜í…ì¸  ë¶„ì„ ì‹œì‘...")
    url = state['url']
    title, text = scrape_web_content(url)
    scraped_content = (title or "") + (text or "")
    failure_keywords = ["ì˜¤ë¥˜ ë°œìƒ", "ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ìŠ¤í¬ë©ì´ ê¸ˆì§€ëœ ê¸€"] 
    if any(k in scraped_content for k in failure_keywords):
        st.error(f"âš ï¸ {scraped_content}")
        return {"scraped_content": f"ë¶„ì„ ì‹¤íŒ¨: {scraped_content}"}
    st.success("âœ… ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ!")
    return {
        "scraped_content": scraped_content,
        "messages": [HumanMessage(content=f"URL '{url}'ì˜ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ.")]
    }


def seo_specialist_node(state: AgentState):
    st.write("â–¶ï¸ SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ë„¤ì´ë²„ SEO ì „ëµ ë¶„ì„ ì¤‘...")
    scraped_content = state['scraped_content']
    search_query = "2025ë…„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ìµœì í™” ì „ëµ"
    tavily_api_key = st.session_state.get("tavily_api_key")
    if not tavily_api_key:
        st.error("âŒ Tavily API Keyê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {"scraping_status": "Failure", "seo_analysis": "Tavily API Key ì—†ìŒ", "seo_tags": []}

    try:
        tavily = TavilySearch(max_results=3, tavily_api_key=tavily_api_key)
        results = tavily.invoke({"query": search_query})
        seo_trends = ""
        if results and "results" in results:
            for r in results["results"]:
                seo_trends += f"ì œëª©: {r.get('title','')}\në‚´ìš©: {r.get('content','')}\n\n"
        else:
            seo_trends = "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        st.error(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        seo_trends = ""

    prompt = ChatPromptTemplate.from_messages([
        ("system",
         """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
         ì£¼ì–´ì§„ ì›ë³¸ ì½˜í…ì¸ ì™€ ìµœì‹  SEO íŠ¸ë Œë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
         ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
         [ë¶„ì„ ë° ì „ëµ]
         - (ì—¬ê¸°ì— ì½˜í…ì¸  ê¸°ë°˜ SEO ì „ëµê³¼ í‚¤ì›Œë“œ ë¶„ì„ ë‚´ìš©ì„ ì„œìˆ )
         
         [ì¶”ì²œ íƒœê·¸]
         íƒœê·¸1, íƒœê·¸2, íƒœê·¸3, ... (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ 30ê°œì˜ íƒœê·¸)"""),
        ("human",
         "**ìµœì‹  ë„¤ì´ë²„ SEO íŠ¸ë Œë“œ:**\n{seo_trends}\n\n"
         "**ë¶„ì„í•  ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}")
    ])

    llm = get_llm()
    if llm is None:
        return {"scraping_status": "Failure", "seo_analysis": "LLM ì—†ìŒ", "seo_tags": []}

    chain = prompt | llm
    resp = chain.invoke({"seo_trends": seo_trends, "scraped_content": scraped_content[:4000]})
    analysis_text = resp.content
    try:
        tags_part = analysis_text.split("[ì¶”ì²œ íƒœê·¸]")[1].strip()
        tags = [t.strip() for t in tags_part.split(",") if t.strip()]
    except IndexError:
        tags = []

    st.success("âœ… SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ì „ëµ ë¶„ì„ ë° íƒœê·¸ ìƒì„± ì™„ë£Œ!")
    return {"seo_analysis": analysis_text, "seo_tags": tags}


def writer_node(state: AgentState):
    st.write("â–¶ï¸ ì‘ì„±ê°€ ì—ì´ì „íŠ¸: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    scraped_content = state['scraped_content']
    seo_analysis = state['seo_analysis']

    title_prompt = ChatPromptTemplate.from_template(
        "ì£¼ì–´ì§„ SEO ë¶„ì„ê³¼ ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ì œëª© 1ê°œë§Œ ìƒì„±í•´ì£¼ì„¸ìš”. (ì¶”ê°€ ì„¤ëª… ì—†ì´ ì œëª©ë§Œ ì¶œë ¥)\n\n"
        "**SEO ë¶„ì„:**\n{seo_analysis}\n\n"
        "**ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"
    )
    llm = get_llm()
    if llm is None:
        return {"draft_post": "LLM ì—†ìŒ", "final_title": "", "final_subheadings": [], "naver_seo_subtitles": []}

    title_chain = title_prompt | llm
    main_title = title_chain.invoke({
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:2000]
    }).content.strip().replace('"', '')

    subtitle_prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¸”ë¡œê·¸ ì œëª©ê³¼ SEO ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEOì— ìµœì í™”ëœ ë¶€ì œëª© 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        **ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë¶€ì œëª© ì‘ì„± ê°€ì´ë“œ:**
        - ê²€ìƒ‰ í‚¤ì›Œë“œ ìì—°ìŠ¤ëŸ¬ìš´ í¬í•¨
        - í´ë¦­ì„ ìœ ë„í•˜ëŠ” ë¬¸êµ¬
        - ìˆ«ì/ì‹œê°„ í‘œí˜„ í™œìš©
        - ê°ì •ì /í˜¸ê¸°ì‹¬ ìœ ë°œ ìš”ì†Œ
        - 20-30ì ë‚´ì™¸
        - ì„œë¡œ ë‹¤ë¥¸ ê´€ì 

        ê° ë¶€ì œëª©ì€ í•œ ì¤„ì”© ë²ˆí˜¸ ì—†ì´ ì¶œë ¥í•˜ì„¸ìš”.
        
        **ë©”ì¸ ì œëª©:** {main_title}
        
        **SEO ë¶„ì„:**
        {seo_analysis}"""
    )
    subtitle_chain = subtitle_prompt | llm
    subtitle_resp = subtitle_chain.invoke({"main_title": main_title, "seo_analysis": seo_analysis}).content
    naver_seo_subtitles = [ln.strip() for ln in subtitle_resp.split("\n") if ln.strip() and not ln.strip().startswith("**")]

    draft_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì œëª©, SEO ë¶„ì„, ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ì¹œê·¼í•œ ì–´ì¡°ì˜ ë§¤ë ¥ì ì¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‚´ìš©ì€ ì„œë¡ , ë³¸ë¡ (ì†Œì œëª© ## ì‚¬ìš©), ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”."),
        ("human", "**ì œëª©:** {main_title}\n\n**SEO ë¶„ì„:**\n{seo_analysis}\n\n**ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}")
    ])
    draft_chain = draft_prompt | llm
    draft_post = draft_chain.invoke({
        "main_title": main_title,
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:4000]
    }).content

    subheadings = [ln.replace("## ", "").strip() for ln in draft_post.split("\n") if ln.startswith("## ")]

    st.success("âœ… ì‘ì„±ê°€ ì—ì´ì „íŠ¸: í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ!")
    return {
        "draft_post": draft_post,
        "final_title": main_title,
        "final_subheadings": subheadings,
        "naver_seo_subtitles": naver_seo_subtitles
    }


def art_director_node(state: AgentState):
    st.write("â–¶ï¸ ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    title = state['final_title']
    subtitles = state.get('naver_seo_subtitles', [])

    if not st.session_state.get("openai_api_key"):
        st.warning("âš ï¸ ì´ë¯¸ì§€ ìƒì„±(DALL-E)ì„ ìœ„í•´ì„œëŠ” OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return {"image_prompt": "", "image_url": "", "subtitle_image_prompts": [], "subtitle_image_urls": [], "image_keywords": []}

    prompt_llm = get_llm()
    if prompt_llm is None:
        return {"image_prompt": "", "image_url": "", "subtitle_image_prompts": [], "subtitle_image_urls": [], "image_keywords": []}

    keyword_t = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ë¸”ë¡œê·¸ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 2ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì–¸ë”ìŠ¤ì½”ì–´(_)ë¡œ ì—°ê²°í•´ì„œ ì¶œë ¥í•˜ì„¸ìš”.\nì˜ˆ: 'ë§›ì§‘_í›„ê¸°' ë˜ëŠ” 'ì—¬í–‰_íŒ'\nì œëª©: {title}"
    )
    kw_chain = keyword_t | prompt_llm
    kw_resp = kw_chain.invoke({"title": title}).content.strip()
    image_keywords = [k.strip() for k in kw_resp.split("_")[:2]]

    try:
        client = OpenAI(api_key=st.session_state.get("openai_api_key"))

        st.write("  ğŸ“¸ ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        main_t = ChatPromptTemplate.from_template("ë¸”ë¡œê·¸ ì œëª© '{title}'ì— ì–´ìš¸ë¦¬ëŠ” DALL-E 3 ì´ë¯¸ì§€ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.")
        main_chain = main_t | prompt_llm
        main_prompt = main_chain.invoke({"title": title}).content

        main_res = client.images.generate(model="dall-e-3", prompt=main_prompt, size="1024x1024", quality="standard", n=1)
        main_url = main_res.data[0].url

        st.write("  ğŸ¨ ë¶€ì œëª© ê¸°ë°˜ ì´ë¯¸ì§€ 3ê°œ ìƒì„± ì¤‘...")
        sub_prompts, sub_urls = [], []
        for i, sub in enumerate(subtitles[:3], 1):
            st.write(f"    â€¢ ì´ë¯¸ì§€ {i+1} ìƒì„± ì¤‘...")
            sub_t = ChatPromptTemplate.from_template("ë¸”ë¡œê·¸ ë¶€ì œëª© '{subtitle}'ì— ì–´ìš¸ë¦¬ëŠ” DALL-E 3 ì´ë¯¸ì§€ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.")
            sub_chain = sub_t | prompt_llm
            sub_prompt = sub_chain.invoke({"subtitle": sub}).content
            sub_prompts.append(sub_prompt)
            sub_res = client.images.generate(model="dall-e-3", prompt=sub_prompt, size="1024x1024", quality="standard", n=1)
            sub_urls.append(sub_res.data[0].url)

        st.success("âœ… ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ 4ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
        return {
            "image_prompt": main_prompt,
            "image_url": main_url,
            "subtitle_image_prompts": sub_prompts,
            "subtitle_image_urls": sub_urls,
            "image_keywords": image_keywords
        }

    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "image_prompt": locals().get("main_prompt", ""),
            "image_url": "",
            "subtitle_image_prompts": [],
            "subtitle_image_urls": [],
            "image_keywords": image_keywords
        }


def should_continue(state: AgentState):
    if "ë¶„ì„ ì‹¤íŒ¨:" in state.get('scraped_content', ''):
        return "end_process"
    else:
        return "continue_to_seo"


def build_graph():
    workflow = StateGraph(AgentState)
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("seo_specialist", seo_specialist_node)
    workflow.add_node("writer", writer_node)
    workflow.add_node("art_director", art_director_node)

    workflow.set_entry_point("researcher")
    workflow.add_conditional_edges(
        "researcher",
        should_continue,
        {"continue_to_seo": "seo_specialist", "end_process": END}
    )
    workflow.add_edge("seo_specialist", "writer")
    workflow.add_edge("writer", "art_director")
    workflow.add_edge("art_director", END)
    return workflow.compile()

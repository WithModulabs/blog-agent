import streamlit as st
from typing import List, TypedDict

from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.graph import StateGraph, END
from openai import OpenAI

# tools.pyì—ì„œ í•¨ìˆ˜ ì„í¬íŠ¸
from tools import get_llm, scrape_web_content

# --- ì—ì´ì „íŠ¸ ìƒíƒœ ì •ì˜ ---
class AgentState(TypedDict):
    url: str
    scraped_content: str
    seo_analysis: str
    seo_tags: List[str]
    draft_post: str
    final_title: str
    final_subheadings: List[str]
    naver_seo_subtitles: List[str]
    image_prompt: str
    image_url: str
    subtitle_image_prompts: List[str]
    subtitle_image_urls: List[str]
    image_keywords: List[str]
    messages: List[BaseMessage]

# --- ì—ì´ì „íŠ¸ ë° ë…¸ë“œ ì •ì˜ ---
def researcher_node(state: AgentState):
    st.write("â–¶ï¸ ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: URL ì½˜í…ì¸  ë¶„ì„ ì‹œì‘...")
    url = state['url']
    scraped_content = scrape_web_content(url)
    
    failure_keywords = ["ì˜¤ë¥˜ ë°œìƒ", "ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ìŠ¤í¬ë©ì´ ê¸ˆì§€ëœ ê¸€"] 
    
    if any(keyword in scraped_content for keyword in failure_keywords):
        st.error(f"âš ï¸ {scraped_content}")
        return {
            "scraped_content": f"ë¶„ì„ ì‹¤íŒ¨: {scraped_content}",
        }
        
    st.success("âœ… ë¦¬ì„œì²˜ ì—ì´ì „íŠ¸: ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ!")
    return {
        "scraped_content": scraped_content,
        "messages": [HumanMessage(content=f"URL '{url}'ì˜ ì½˜í…ì¸  ë¶„ì„ ì™„ë£Œ.")]
    }

def seo_specialist_node(state: AgentState):
    st.write("â–¶ï¸ SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ë„¤ì´ë²„ SEO ì „ëµ ë¶„ì„ ì¤‘...")
    scraped_content = state['scraped_content']
    search_query = "2025ë…„ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ìµœì í™” ì „ëµ"
    tavily_api_key = st.session_state.get("tavily_api_key")

    if not tavily_api_key:
        st.error("âŒ Tavily API Keyê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {"scraping_status": "Failure", "seo_analysis": "Tavily API Key ì—†ìŒ", "seo_tags": []}

    try:
        tavily_tool = TavilySearchResults(max_results=3, tavily_api_key=tavily_api_key)
        seo_trends = tavily_tool.invoke({"query": search_query})
    except Exception as e:
        st.error(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        seo_trends = ""

    prompt = ChatPromptTemplate.from_messages([
         ("system", 
         """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEO ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
         ì£¼ì–´ì§„ ì›ë³¸ ì½˜í…ì¸ ì™€ ìµœì‹  SEO íŠ¸ë Œë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
         ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:
         [ë¶„ì„ ë° ì „ëµ]
         - (ì—¬ê¸°ì— ì½˜í…ì¸  ê¸°ë°˜ SEO ì „ëµê³¼ í‚¤ì›Œë“œ ë¶„ì„ ë‚´ìš©ì„ ì„œìˆ )
         
         [ì¶”ì²œ íƒœê·¸]
         íƒœê·¸1, íƒœê·¸2, íƒœê·¸3, ... (ì‰¼í‘œë¡œ êµ¬ë¶„ëœ 30ê°œì˜ íƒœê·¸)"""),
        ("human", 
         "**ìµœì‹  ë„¤ì´ë²„ SEO íŠ¸ë Œë“œ:**\n{seo_trends}\n\n"
         "**ë¶„ì„í•  ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"),
    ])
    
    llm = get_llm()
    if llm is None:
        return {"scraping_status": "Failure", "seo_analysis": "LLM ì—†ìŒ", "seo_tags": []}

    chain = prompt | llm
    response = chain.invoke({
        "seo_trends": seo_trends,
        "scraped_content": scraped_content[:4000]
    })
    
    analysis_text = response.content
    try:
        tags_part = analysis_text.split("[ì¶”ì²œ íƒœê·¸]")[1].strip()
        tags = [tag.strip() for tag in tags_part.split(",") if tag.strip()]
    except IndexError:
        tags = []
    
    st.success("âœ… SEO ì „ë¬¸ê°€ ì—ì´ì „íŠ¸: ì „ëµ ë¶„ì„ ë° íƒœê·¸ ìƒì„± ì™„ë£Œ!")
    return {"seo_analysis": analysis_text, "seo_tags": tags}

def writer_node(state: AgentState):
    st.write("â–¶ï¸ ì‘ì„±ê°€ ì—ì´ì „íŠ¸: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì¤‘...")
    scraped_content = state['scraped_content']
    seo_analysis = state['seo_analysis']
    
    title_prompt = ChatPromptTemplate.from_template(
        "ì£¼ì–´ì§„ SEO ë¶„ì„ê³¼ ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ê²€ìƒ‰ì— ìµœì í™”ëœ ë§¤ë ¥ì ì¸ ë¸”ë¡œê·¸ ì œëª© 1ê°œë§Œ ìƒì„±í•´ì£¼ì„¸ìš”. (ì¶”ê°€ ì„¤ëª… ì—†ì´ ì œëª©ë§Œ ì¶œë ¥)\n\n"
        "**SEO ë¶„ì„:**\n{seo_analysis}\n\n"
        "**ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content}"
    )
    
    llm = get_llm()
    if llm is None:
        return {"draft_post": "LLM ì—†ìŒ", "final_title": "", "final_subheadings": [], "naver_seo_subtitles": []}

    title_chain = title_prompt | llm
    main_title = title_chain.invoke({
        "seo_analysis": seo_analysis,
        "scraped_content": scraped_content[:2000]
    }).content.strip().replace('"', '')

    # ë„¤ì´ë²„ SEO ìµœì í™” ë¶€ì œëª© ìƒì„±
    subtitle_prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¸”ë¡œê·¸ ì œëª©ê³¼ SEO ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, ë„¤ì´ë²„ ë¸”ë¡œê·¸ SEOì— ìµœì í™”ëœ ë¶€ì œëª© 5ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        
        **ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë¶€ì œëª© ì‘ì„± ê°€ì´ë“œ:**
        - ê²€ìƒ‰ í‚¤ì›Œë“œê°€ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ë˜ë„ë¡ ì‘ì„±
        - í´ë¦­ì„ ìœ ë„í•˜ëŠ” ë§¤ë ¥ì ì¸ ë¬¸êµ¬ ì‚¬ìš© (ì˜ˆ: "ê¼­ ì•Œì•„ì•¼ í• ", "ì™„ë²½ ê°€ì´ë“œ", "ì‹¤ì œ í›„ê¸°")
        - êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ì‹œê°„ í‘œí˜„ í¬í•¨ (ì˜ˆ: "3ê°€ì§€ ë°©ë²•", "10ë¶„ ë§Œì—", "2025ë…„ ìµœì‹ ")
        - ê°ì •ì  ì–´í•„ì´ë‚˜ í˜¸ê¸°ì‹¬ ìœ ë°œ ìš”ì†Œ ì¶”ê°€
        - 20-30ì ë‚´ì™¸ë¡œ ì ì ˆí•œ ê¸¸ì´ ìœ ì§€
        - ê° ë¶€ì œëª©ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì¸¡ë©´ì„ ë‹¤ë£¨ë„ë¡ êµ¬ì„±
        
        ê° ë¶€ì œëª©ì€ í•œ ì¤„ì”© ë²ˆí˜¸ ì—†ì´ ì¶œë ¥í•˜ì„¸ìš”.
        
        **ë©”ì¸ ì œëª©:** {main_title}
        
        **SEO ë¶„ì„:**
        {seo_analysis}"""
    )
    
    subtitle_chain = subtitle_prompt | llm
    subtitle_response = subtitle_chain.invoke({
        "main_title": main_title,
        "seo_analysis": seo_analysis
    }).content
    
    naver_seo_subtitles = [line.strip() for line in subtitle_response.split('\n') if line.strip() and not line.strip().startswith('**')]

    draft_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì „ë¬¸ ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì œëª©, SEO ë¶„ì„, ì›ë³¸ ì½˜í…ì¸ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ëª¨ì§€ë¥¼ í™œìš©í•˜ì—¬ ì¹œê·¼í•œ ì–´ì¡°ì˜ ë§¤ë ¥ì ì¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ë‚´ìš©ì€ ì„œë¡ , ë³¸ë¡ (ì†Œì œëª© ## ì‚¬ìš©), ê²°ë¡ ìœ¼ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”."),
        ("human", f"**ì œëª©:** {main_title}\n\n**SEO ë¶„ì„:**\n{seo_analysis}\n\n**ì›ë³¸ ì½˜í…ì¸ :**\n{scraped_content[:4000]}"),
    ])
    
    draft_chain = draft_prompt | llm
    draft_post = draft_chain.invoke({}).content
    
    subheadings = [line.replace('## ', '').strip() for line in draft_post.split('\n') if line.startswith('## ')]
            
    st.success("âœ… ì‘ì„±ê°€ ì—ì´ì „íŠ¸: í¬ìŠ¤íŠ¸ ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ!")
    return {"draft_post": draft_post, "final_title": main_title, "final_subheadings": subheadings, "naver_seo_subtitles": naver_seo_subtitles}

def art_director_node(state: AgentState):
    st.write("â–¶ï¸ ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
    title = state['final_title']
    subtitles = state.get('naver_seo_subtitles', [])
    
    # ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ì„œëŠ” OpenAI í‚¤ê°€ ë°˜ë“œì‹œ í•„ìš”
    if not st.session_state.get("openai_api_key"):
        st.warning("âš ï¸ ì´ë¯¸ì§€ ìƒì„±(DALL-E)ì„ ìœ„í•´ì„œëŠ” OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return {"image_prompt": "", "image_url": "", "subtitle_image_prompts": [], "subtitle_image_urls": [], "image_keywords": []}

    # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìƒì„±ì€ ì‚¬ìš©ìê°€ ì„ íƒí•œ LLM ì‚¬ìš©
    prompt_generator_llm = get_llm()
    if prompt_generator_llm is None:
        return {"image_prompt": "", "image_url": "", "subtitle_image_prompts": [], "subtitle_image_urls": [], "image_keywords": []}

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    keyword_template = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ë¸”ë¡œê·¸ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 2ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì–¸ë”ìŠ¤ì½”ì–´(_)ë¡œ ì—°ê²°í•´ì„œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        "ì˜ˆ: 'ë§›ì§‘_í›„ê¸°' ë˜ëŠ” 'ì—¬í–‰_íŒ' í˜•ì‹ìœ¼ë¡œ\n"
        "ì œëª©: {title}"
    )
    keyword_chain = keyword_template | prompt_generator_llm
    keywords_response = keyword_chain.invoke({"title": title}).content.strip()
    image_keywords = [kw.strip() for kw in keywords_response.split('_')[:2]]
    
    try:
        client = OpenAI(api_key=st.session_state.get("openai_api_key"))
        
        # 1. ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± (ì œëª© ê¸°ë°˜)
        st.write("  ğŸ“¸ ë©”ì¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        main_prompt_template = ChatPromptTemplate.from_template("ë¸”ë¡œê·¸ ì œëª© '{title}'ì— ì–´ìš¸ë¦¬ëŠ” DALL-E 3 ì´ë¯¸ì§€ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.")
        main_chain = main_prompt_template | prompt_generator_llm
        main_image_prompt = main_chain.invoke({"title": title}).content
        
        main_response = client.images.generate(
            model="dall-e-3", prompt=main_image_prompt, size="1024x1024", quality="standard", n=1
        )
        main_image_url = main_response.data[0].url
        
        # 2. ë¶€ì œëª© ê¸°ë°˜ ì´ë¯¸ì§€ 3ê°œ ìƒì„±
        st.write("  ğŸ¨ ë¶€ì œëª© ê¸°ë°˜ ì´ë¯¸ì§€ 3ê°œ ìƒì„± ì¤‘...")
        subtitle_prompts = []
        subtitle_urls = []
        
        for i, subtitle in enumerate(subtitles[:3], 1):
            st.write(f"    â€¢ ì´ë¯¸ì§€ {i+1} ìƒì„± ì¤‘...")
            subtitle_prompt_template = ChatPromptTemplate.from_template(
                "ë¸”ë¡œê·¸ ë¶€ì œëª© '{subtitle}'ì— ì–´ìš¸ë¦¬ëŠ” DALL-E 3 ì´ë¯¸ì§€ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜."
            )
            subtitle_chain = subtitle_prompt_template | prompt_generator_llm
            subtitle_image_prompt = subtitle_chain.invoke({"subtitle": subtitle}).content
            subtitle_prompts.append(subtitle_image_prompt)
            
            subtitle_response = client.images.generate(
                model="dall-e-3", prompt=subtitle_image_prompt, size="1024x1024", quality="standard", n=1
            )
            subtitle_urls.append(subtitle_response.data[0].url)
        
        st.success("âœ… ì•„íŠ¸ ë””ë ‰í„° ì—ì´ì „íŠ¸: ì´ 4ê°œ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
        return {
            "image_prompt": main_image_prompt, 
            "image_url": main_image_url,
            "subtitle_image_prompts": subtitle_prompts,
            "subtitle_image_urls": subtitle_urls,
            "image_keywords": image_keywords
        }
        
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "image_prompt": main_image_prompt if 'main_image_prompt' in locals() else "", 
            "image_url": "",
            "subtitle_image_prompts": [],
            "subtitle_image_urls": [],
            "image_keywords": image_keywords
        }

# --- ê·¸ë˜í”„ ë¹Œë“œ ---
def should_continue(state: AgentState):
    if "ë¶„ì„ ì‹¤íŒ¨:" in state.get('scraped_content', ''):
        return "end_process"
    else:
        return "continue_to_seo"

def build_graph():
    workflow = StateGraph(AgentState)
    
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("seo_specialist", seo_specialist_node)
    workflow.add_node("writer", writer_node)
    workflow.add_node("art_director", art_director_node)
    
    workflow.set_entry_point("researcher")
    workflow.add_conditional_edges(
        "researcher",
        should_continue,
        {"continue_to_seo": "seo_specialist", "end_process": END}
    )
    workflow.add_edge("seo_specialist", "writer")
    workflow.add_edge("writer", "art_director")
    workflow.add_edge("art_director", END)
    
    return workflow.compile()